{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ChatBot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNp3Gr675IhAcZR0XnVrM+q"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started"
      ],
      "metadata": {
        "id": "3F5P8wso5guj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "Installing required libraries and Importing Packages\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "01oABDDV56md"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7iMcfaBQCzG",
        "outputId": "012486e0-5b6a-4289-a0c4-af58223f6641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import json\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import random\n"
      ],
      "metadata": {
        "id": "5_BuOyu_S2VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBwyXEkFW8c1",
        "outputId": "2114d7c7-203c-4703-d039-3fab4835847e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Dii4mKXCsr",
        "outputId": "d5a9b9cd-4dd3-4b9d-9b55-7fa5c6277891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kks4XdpEXLEu",
        "outputId": "cda0674f-1db4-4ec2-955c-09529642cf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JBEAFUaXUeJ",
        "outputId": "5e752efa-e10e-4107-8ae3-fd149bd8624b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.47.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creatng dataset from intents file"
      ],
      "metadata": {
        "id": "1fP2TCk76aew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[]\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?', '!']\n",
        "data_file = open('intents.json').read()\n",
        "intents = json.loads(data_file)\n",
        "\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokenize each word\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "\n",
        "        #add documents in the corpus\n",
        "        documents.append((w, intent['tag']))\n",
        "\n",
        "        # add tags to classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])\n",
        "\n",
        "# lemmaztize and lower each word and remove duplicates\n",
        "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# sort classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "# documents = combination between patterns and intents\n",
        "print (len(documents), \"documents\")\n",
        "\n",
        "# classes = intents\n",
        "print (len(classes), \"classes\", classes)\n",
        "\n",
        "# words = all words, vocabulary\n",
        "print (len(words), \"unique lemmatized words\", words)\n",
        "\n",
        "\n",
        "pickle.dump(words,open('words.pkl','wb'))\n",
        "pickle.dump(classes,open('classes.pkl','wb'))\n",
        "\n",
        "# create our training data\n",
        "training = []\n",
        "\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # lemmatize each word - create base word, in attempt to represent related words\n",
        "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array with 1, if word match found in current pattern\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "    \n",
        "    # output is a '0' for each tag and '1' for current tag (for each pattern)\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "# create train lists. X - patterns, Y - intents\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n",
        "print(\"Training data is created\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JjpVEdQWquN",
        "outputId": "d434687e-2589-41be-972b-ea4837aa5571"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163 documents\n",
            "23 classes ['', 'Clever', 'CourtesyGoodBye', 'CourtesyGreeting', 'CourtesyGreetingResponse', 'CurrentHumanQuery', 'Gali', 'Gossip', 'GreetingResponse', 'Jokes', 'Love', 'NameQuery', 'NotTalking2U', 'PodBayDoor', 'SelfAware', 'Shutup', 'Thanks', 'UnderstandQuery', 'age', 'goodbye', 'greeting', 'options', 'riddle']\n",
            "144 unique lemmatized words [\"'s\", ',', 'a', 'about', 'adam', 'age', 'am', 'any', 'anyone', 'are', 'ask', 'asshole', 'aulad', 'aware', 'bay', 'bc', 'be', 'bella', 'bhenchod', 'bhosdike', 'bkl', 'bored', 'bosdike', 'bsdk', 'by', 'bye', 'call', 'can', 'cao', 'cheering', 'clever', 'communicating', 'comprendo', 'conscious', 'could', 'cya', 'date', 'day', 'do', 'doing', 'door', 'enough', 'for', 'friend', 'fuck', 'gaand', 'gandu', 'genious', 'get', 'girl', 'give', 'good', 'goodbye', 'gossip', 'got', 'great', 'greeting', 'have', 'hear', 'hello', 'help', 'helpful', 'hey', 'hi', 'hola', 'hope', 'how', 'hya', 'i', 'intelligent', 'is', 'it', 'joke', 'ki', 'know', 'later', 'laugh', 'leaving', 'love', 'madarchod', 'made', 'make', 'marra', 'marry', 'mc', 'me', 'mean', 'meant', 'more', 'my', \"n't\", 'name', 'need', 'not', 'offered', 'ok', 'old', 'open', 'please', 'pod', 'prove', 'provide', 'question', 'quiet', 'randi', 'riddle', 'saying', 'see', 'self', 'self-aware', 'shhh', 'shut', 'some', 'speaking', 'stop', 'support', 'talking', 'tell', 'thank', 'thanks', 'that', 'the', 'there', 'think', 'this', 'time', 'to', 'understand', 'up', 'user', 'very', 'wa', 'want', 'well', 'were', 'what', 'whats', 'when', 'who', 'will', 'with', 'ya', 'you', 'your']\n",
            "Training data is created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:68: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building and tuning the model"
      ],
      "metadata": {
        "id": "fnZvepaH_h9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model"
      ],
      "metadata": {
        "id": "CjV7R9bj_pej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_model(mods):\n",
        "\n",
        "  model=Sequential()\n",
        "  model.add(Dense(mods.Int('units_1', min_value=16, max_value=1024,step=16),activation=mods.Choice(\"activation_1\", [\"relu\",\"elu\" ,\"tanh\",\"selu\"]), input_shape=(len(train_x[0]),), kernel_regularizer=tf.keras.regularizers.l2(mods.Float('l2', 1e-6, 1e-2,sampling='log'))))\n",
        "  model.add(Dropout(mods.Choice('rate_1', [ 0.0, 0.1, 0.2, 0.3, 0.4,0.5,0.6,0.7 ]))) \n",
        "  model.add(Dense(mods.Int('units_2', min_value=16, max_value=1024,step=16),activation=mods.Choice(\"activation_2\", [\"relu\",\"elu\" ,\"tanh\",\"selu\"]),kernel_regularizer=tf.keras.regularizers.l2(mods.Float('l2', 1e-6, 1e-2,sampling='log'))))\n",
        "  model.add(Dropout(mods.Choice('rate_2', [ 0.0, 0.1, 0.2, 0.3, 0.4,0.5,0.6,0.7 ]))) \n",
        "  model.add(Dense(mods.Int('units_3', min_value=16, max_value=1024,step=16),activation=mods.Choice(\"activation_3\", [\"relu\",\"elu\" ,\"tanh\",\"selu\"]),kernel_regularizer=tf.keras.regularizers.l2(mods.Float('l2', 1e-6, 1e-2,sampling='log'))))\n",
        "  model.add(Dropout(mods.Choice('rate_3', [ 0.0, 0.1, 0.2, 0.3, 0.4,0.5,0.6,0.7 ]))) \n",
        "  model.add(Dense(len(train_y[0]),activation=mods.Choice(\"activation_4\", [\"relu\",\"elu\" ,\"tanh\",\"selu\"]),kernel_regularizer=tf.keras.regularizers.l2(mods.Float('l2', 1e-6, 1e-2,sampling='log'))))\n",
        "  model.add(Dropout(mods.Choice('rate_4', [ 0.0, 0.1, 0.2, 0.3, 0.4,0.5,0.6,0.7 ]))) \n",
        "  sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=sgd,metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "9uVTUZpGXycK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing keras tuner and tuning the hyperparmeters"
      ],
      "metadata": {
        "id": "UjhqxW9J_wPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt\n",
        "import datetime\n",
        "# run parameter\n",
        "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%m%d-%H%M\")\n",
        "\n",
        "tc = tf.keras.callbacks.TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    embeddings_freq=1,\n",
        "    write_graph=True,\n",
        "    update_freq='batch')\n",
        "tuner=kt.RandomSearch(make_model,objective = 'accuracy', max_trials=100, overwrite=True )\n",
        "\n",
        "tuner.search(np.array(train_x), np.array(train_y), epochs=200, batch_size=4, verbose=1, callbacks=[tc])\n",
        "tuner.search_space_summary()\n",
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GuxjHKqYS6e",
        "outputId": "51dff3f1-9a12-49ad-a715-97834788d61e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 26 Complete [00h 02m 23s]\n",
            "accuracy: 0.042944785207509995\n",
            "\n",
            "Best accuracy So Far: 0.6809815764427185\n",
            "Total elapsed time: 01h 11m 16s\n",
            "\n",
            "Search: Running Trial #27\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "192               |176               |units_1\n",
            "selu              |tanh              |activation_1\n",
            "0.00019873        |9.8121e-05        |l2\n",
            "0.7               |0.1               |rate_1\n",
            "512               |912               |units_2\n",
            "elu               |tanh              |activation_2\n",
            "0.2               |0                 |rate_2\n",
            "576               |560               |units_3\n",
            "selu              |selu              |activation_3\n",
            "0.6               |0                 |rate_3\n",
            "selu              |selu              |activation_4\n",
            "0.3               |0                 |rate_4\n",
            "\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 192)               27840     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 192)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               98816     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 576)               295488    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 576)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 23)                13271     \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 23)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 435,415\n",
            "Trainable params: 435,415\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "41/41 [==============================] - 2s 20ms/step - loss: 10.4318 - accuracy: 0.0245\n",
            "Epoch 2/200\n",
            "41/41 [==============================] - 1s 24ms/step - loss: 9.7829 - accuracy: 0.0491\n",
            "Epoch 3/200\n",
            "41/41 [==============================] - 1s 26ms/step - loss: 8.6251 - accuracy: 0.0613\n",
            "Epoch 4/200\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 7.6623 - accuracy: 0.0798\n",
            "Epoch 5/200\n",
            "41/41 [==============================] - 1s 18ms/step - loss: 8.0759 - accuracy: 0.0675\n",
            "Epoch 6/200\n",
            "41/41 [==============================] - 1s 18ms/step - loss: 8.6448 - accuracy: 0.0613\n",
            "Epoch 7/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 7.9752 - accuracy: 0.0613\n",
            "Epoch 8/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 8.4724 - accuracy: 0.0368\n",
            "Epoch 9/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 8.7626 - accuracy: 0.0184\n",
            "Epoch 10/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 8.0558 - accuracy: 0.0245\n",
            "Epoch 11/200\n",
            "41/41 [==============================] - 1s 18ms/step - loss: 7.6955 - accuracy: 0.0675\n",
            "Epoch 12/200\n",
            "41/41 [==============================] - 1s 20ms/step - loss: 7.2113 - accuracy: 0.0307\n",
            "Epoch 13/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 7.8175 - accuracy: 0.0307\n",
            "Epoch 14/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 7.7624 - accuracy: 0.0552\n",
            "Epoch 15/200\n",
            "41/41 [==============================] - 1s 19ms/step - loss: 6.9527 - accuracy: 0.0491\n",
            "Epoch 16/200\n",
            "16/41 [==========>...................] - ETA: 0s - loss: 7.3802 - accuracy: 0.0781"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorboard"
      ],
      "metadata": {
        "id": "jNrj1smenYNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "zZEW3biGngUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir 'logs/'"
      ],
      "metadata": {
        "id": "sbej2aSAni9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plt_metric(history, metric, title, has_valid=True):\n",
        "    \"\"\"Plots the given 'metric' from 'history'.\n",
        "\n",
        "    Arguments:\n",
        "        history: history attribute of History object returned from Model.fit.\n",
        "        metric: Metric to plot, a string value present as key in 'history'.\n",
        "        title: A string to be used as title of plot.\n",
        "        has_valid: Boolean, true if valid data was passed to Model.fit else false.\n",
        "\n",
        "    Returns:\n",
        "        None.\n",
        "    \"\"\"\n",
        "    plt.plot(history[metric])\n",
        "    if has_valid:\n",
        "        plt.plot(history[\"val_\" + metric])\n",
        "        plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "    plt.title(title)\n",
        "    plt.ylabel(metric)\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "S2CXbHl4UOdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_trials = tuner.get_best_hyperparameters(num_trials=10)\n",
        "i=0\n",
        "for trial in best_trials:\n",
        "    model = tuner.hypermodel.build(trial)\n",
        "    history = model.fit(np.array(train_x), np.array(train_y), validation_split=0.2, epochs=500, batch_size=5, verbose=1 ,callbacks=[ec])\n",
        "    # Plot the accuracy\n",
        "    plt_metric(history=history.history, metric=\"accuracy\", title=\"Model accuracy\")\n",
        "    # Plot the loss\n",
        "    plt_metric(history=history.history, metric=\"loss\", title=\"Loss\")\n",
        "    i+=1\n",
        "    model.save(str(i))"
      ],
      "metadata": {
        "id": "P3X3oLSPUb-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('1')\n",
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern - split words into array\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word - create short form for word\n",
        "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "\n",
        "def bow(sentence, words, show_details=True):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words - matrix of N words, vocabulary matrix\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                # assign 1 if current word is in the vocabulary position\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "    return(np.array(bag))\n",
        "\n",
        "def predict_class(sentence, model):\n",
        "    # filter out predictions below a threshold\n",
        "    p = bow(sentence, words,show_details=False)\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "    ERROR_THRESHOLD = 0.25\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "    return return_list\n",
        "\n",
        "def getResponse(ints, intents_json):\n",
        "    tag = ints[0]['intent']\n",
        "    list_of_intents = intents_json['intents']\n",
        "    for i in list_of_intents:\n",
        "        if(i['tag']== tag):\n",
        "            result = random.choice(i['responses'])\n",
        "            break\n",
        "    return result\n",
        "\n",
        "def chatbot_response(msg):\n",
        "    ints = predict_class(msg, model)\n",
        "    res = getResponse(ints, intents)\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "tF-uBlPCYRCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "msg = input(\"you:\")\n",
        "while msg!=\"quit\":\n",
        "  res = chatbot_response(msg)\n",
        "  print(\"ChatBot:\"+res)\n",
        "  \n",
        "  msg = input(\"you:\")"
      ],
      "metadata": {
        "id": "rEla9OwOPiTp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}